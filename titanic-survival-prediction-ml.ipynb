{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic Competition\n\nUpvote my work if you found it useful.\n\nI got the idea for the time series analysis below from the following:\n-https://www.kaggle.com/code/melodyyiphoiching/titanic-machine-learning-from-disaster/edit\n- MANAV SEHGAL(https://www.kaggle.com/code/startupsci/titanic-data-science-solutions)\n- SAS - Example: Model Titanic Survival (https://documentation.sas.com/doc/en/vdmmlcdc/8.1/caspg3/n0c8k27l2vkwlin196w5qptqsjqx.htm)\n- AYSE BAT (https://www.kaggle.com/code/aaysbt/titanic-datasets-eda-fe-dc-model-predictions)\n- LD FREEMAN (https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy#Step-7:-Optimize-and-Strategize)\n\n\n## Challenge\n\nBuild a predictive model that answers the question: \"what sorts of people were more likely to survive the Titanic sinking?\"\n\n\n## Data \nTo take a look at the competition data, click on the Data tab at the top of the competition page. Then, scroll down to find the list of files.\nThere are three files in the data: (1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n| Column Name - customers.csv | Description |\n|:--|:--|\n| Survival | Survival (0 = No; 1 = Yes). Not included in test.csv file | \n| Pclass |  Ticket Class/ A Proxy for socio-economic status(SES) (1 = 1st/Upper ; 2 = 2nd/Middle; 3 = 3rd/Lower) | \n| Name | Name |\n| Sex | Sex |\n| Age | Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5 |\n| Sibsp | Number of Siblings (brother, sister, stepbrother, stepsister) /Spouses (husband, wife (mistresses and fianc√©s were ignored)) Aboard |\n| Parch | Number of Parents (mother, father)/Children (daughter, son, stepdaughter, stepson) Aboard; Some children travelled only with a nanny, therefore parch=0 for them. |\n| Ticket | Ticket Number |\n| Fare | Passenger Fare |\n| Cabin | Cabin |\n| Embarked | Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) |\n\n\n## Workflow stages\n\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. [Question or problem definition](#ch1)\n2. [Wrangle, prepare, cleanse the data](#ch2)\n3. [EDA (Exploratory Data Analysis) - Analyze, identify patterns, and explore the data](#ch3)\n4. [Acquire training and testing data](#ch4)\n5. [Model, predict and solve the problem](#ch5)\n6. [Visualize, report, and present the problem solving steps and final solution](#ch6)\n7. [Supply or submit the results](#ch7)","metadata":{}},{"cell_type":"code","source":"# linear algebra\nimport numpy as np \n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import ShuffleSplit\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\n# mpl.style.use('ggplot')\n# sns.set_style('white')\n# pylab.rcParams['figure.figsize'] = 12,8","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.21764Z","iopub.execute_input":"2022-04-27T22:51:26.217921Z","iopub.status.idle":"2022-04-27T22:51:26.239091Z","shell.execute_reply.started":"2022-04-27T22:51:26.217889Z","shell.execute_reply":"2022-04-27T22:51:26.238186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ch1\"></a>\n### 1. Question or problem definition \n\nThe competition is simple: Use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Get the data","metadata":{}},{"cell_type":"code","source":"# Get the data\nimport pandas as pd \n\nsub_df = pd.read_csv(\"../input/titanic/test.csv\")\ndf = pd.read_csv(\"../input/titanic/train.csv\")\n# df = pd.concat([train_df, test_df])\n\ndf.columns.values","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.240718Z","iopub.execute_input":"2022-04-27T22:51:26.240976Z","iopub.status.idle":"2022-04-27T22:51:26.276963Z","shell.execute_reply.started":"2022-04-27T22:51:26.240919Z","shell.execute_reply":"2022-04-27T22:51:26.276326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 11 features and one target variable (survived). \n\n**\"PassengerId\"**, **\"Ticket\"** and **\"Name\"** would be highly correlated with the survival rate as they are unique values for each customer. However, they are too specific to be used for modeling. We need to generalize these fields.","metadata":{}},{"cell_type":"code","source":"print('Descriptive statistics of train_df:\\n')\ndf.describe(include = 'all')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.278339Z","iopub.execute_input":"2022-04-27T22:51:26.278684Z","iopub.status.idle":"2022-04-27T22:51:26.342164Z","shell.execute_reply.started":"2022-04-27T22:51:26.278643Z","shell.execute_reply":"2022-04-27T22:51:26.341302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.343741Z","iopub.execute_input":"2022-04-27T22:51:26.344289Z","iopub.status.idle":"2022-04-27T22:51:26.361299Z","shell.execute_reply.started":"2022-04-27T22:51:26.344257Z","shell.execute_reply":"2022-04-27T22:51:26.360467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.362449Z","iopub.execute_input":"2022-04-27T22:51:26.363297Z","iopub.status.idle":"2022-04-27T22:51:26.381731Z","shell.execute_reply.started":"2022-04-27T22:51:26.363249Z","shell.execute_reply":"2022-04-27T22:51:26.380503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.","metadata":{}},{"cell_type":"code","source":"df.shape, sub_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.383299Z","iopub.execute_input":"2022-04-27T22:51:26.384167Z","iopub.status.idle":"2022-04-27T22:51:26.400652Z","shell.execute_reply.started":"2022-04-27T22:51:26.384092Z","shell.execute_reply":"2022-04-27T22:51:26.399725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe values in the second column (**\"Survived\"**) can be used to determine whether each passenger survived or not: \n\n- if it's a \"1\", the passenger survived.\n- if it's a \"0\", the passenger died.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ch2\"></a>\n### 2. Wrangle, prepare, cleanse the data","metadata":{}},{"cell_type":"markdown","source":"---- Start the data cleaning----\n\nWe need to make sure the data is clean before starting your analysis. As a reminder, we should check for:\n\n- Duplicate records\n- Consistent formatting\n- Missing values\n- Obviously wrong values (x)","metadata":{}},{"cell_type":"markdown","source":"#### Duplicate Records\nHow many duplicate transaction records are there?","metadata":{}},{"cell_type":"code","source":"#Find the number duplicate record\nprint('df - Number of duplicate Record:', df.duplicated().sum())\n\nprint('sub_df - Number of duplicate Record:', sub_df.duplicated().sum())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.402423Z","iopub.execute_input":"2022-04-27T22:51:26.402668Z","iopub.status.idle":"2022-04-27T22:51:26.41893Z","shell.execute_reply.started":"2022-04-27T22:51:26.40264Z","shell.execute_reply":"2022-04-27T22:51:26.418174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Missing Values\nHow many missing values are there?","metadata":{}},{"cell_type":"code","source":"#Find the number of null per each columns\nprint('Columns in df with null values:\\n')\nprint(df)\nprint(\"-\"*30)\n\nprint('Columns in sub_df with null values:\\n')\nprint(sub_df.isnull().sum())\nprint(\"-\"*30)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.420116Z","iopub.execute_input":"2022-04-27T22:51:26.42034Z","iopub.status.idle":"2022-04-27T22:51:26.445029Z","shell.execute_reply.started":"2022-04-27T22:51:26.420314Z","shell.execute_reply":"2022-04-27T22:51:26.444203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combine = [df, sub_df]","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.446557Z","iopub.execute_input":"2022-04-27T22:51:26.447023Z","iopub.status.idle":"2022-04-27T22:51:26.45165Z","shell.execute_reply.started":"2022-04-27T22:51:26.446979Z","shell.execute_reply":"2022-04-27T22:51:26.450754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Converting a categorical feature\n\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.","metadata":{}},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.456039Z","iopub.execute_input":"2022-04-27T22:51:26.456477Z","iopub.status.idle":"2022-04-27T22:51:26.477862Z","shell.execute_reply.started":"2022-04-27T22:51:26.456445Z","shell.execute_reply":"2022-04-27T22:51:26.476755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is misssing value in Age, Cabin and Embarked.","metadata":{}},{"cell_type":"markdown","source":"#### Handle missing value - numerical continuous variable\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).\n\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2","metadata":{}},{"cell_type":"code","source":"grid = sns.FacetGrid(df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:26.4797Z","iopub.execute_input":"2022-04-27T22:51:26.480298Z","iopub.status.idle":"2022-04-27T22:51:28.260659Z","shell.execute_reply.started":"2022-04-27T22:51:26.480251Z","shell.execute_reply":"2022-04-27T22:51:28.259713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.","metadata":{}},{"cell_type":"code","source":"guess_ages = np.zeros((2,3))\nguess_ages","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.261924Z","iopub.execute_input":"2022-04-27T22:51:28.26229Z","iopub.status.idle":"2022-04-27T22:51:28.271629Z","shell.execute_reply.started":"2022-04-27T22:51:28.262258Z","shell.execute_reply":"2022-04-27T22:51:28.270637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.","metadata":{}},{"cell_type":"code","source":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.273562Z","iopub.execute_input":"2022-04-27T22:51:28.274248Z","iopub.status.idle":"2022-04-27T22:51:28.327157Z","shell.execute_reply.started":"2022-04-27T22:51:28.274199Z","shell.execute_reply":"2022-04-27T22:51:28.326306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Handle missing value - categorical variable\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.","metadata":{}},{"cell_type":"code","source":"#Find the value count of train_df['Embarked']\nprint('Value count of Embarked variable in train_df:\\n')\nprint(df['Embarked'].value_counts())\nprint(\"-\"*30)\n\n# Find the mode of train_df['Embarked']\nfreq_port = df.Embarked.dropna().mode()[0]\nprint('Mode of Embarked variable in train_df: ',freq_port)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.329268Z","iopub.execute_input":"2022-04-27T22:51:28.330221Z","iopub.status.idle":"2022-04-27T22:51:28.341264Z","shell.execute_reply.started":"2022-04-27T22:51:28.330172Z","shell.execute_reply":"2022-04-27T22:51:28.340388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fill the null value of Embarked with the most common occurance\n\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.344984Z","iopub.execute_input":"2022-04-27T22:51:28.345527Z","iopub.status.idle":"2022-04-27T22:51:28.364959Z","shell.execute_reply.started":"2022-04-27T22:51:28.345488Z","shell.execute_reply":"2022-04-27T22:51:28.363821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Converting categorical feature to numeric\n\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map({\"S\": 1, \"C\": 2, \"Q\": 3})\n    \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.366522Z","iopub.execute_input":"2022-04-27T22:51:28.367012Z","iopub.status.idle":"2022-04-27T22:51:28.388841Z","shell.execute_reply.started":"2022-04-27T22:51:28.366964Z","shell.execute_reply":"2022-04-27T22:51:28.388204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Handle missing value in Fare\nFare feature is fractional value. Our training dataset has one missing values. We simply fill these with the mean.","metadata":{}},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Fare'].fillna(dataset['Fare'].dropna().mean(), inplace=True)\n    dataset['Fare'] = dataset['Fare'].astype(np.int64)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.39025Z","iopub.execute_input":"2022-04-27T22:51:28.390683Z","iopub.status.idle":"2022-04-27T22:51:28.398191Z","shell.execute_reply.started":"2022-04-27T22:51:28.390649Z","shell.execute_reply":"2022-04-27T22:51:28.397227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating new feature extracting from existing (Add Computed Column)\n\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n\nIn the following code we extract Title feature using regular expressions. The RegEx pattern `(\\w+\\.)` matches the first word which ends with a dot character within Name feature. The `expand=False` flag returns a DataFrame.\n\n**Observations.**\n\nWhen we plot Title, Age, and Survived, we note the following observations.\n\n- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n- Survival among Title Age bands varies slightly.\n- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).\n\n**Decision.**\n\n- We decide to retain the new Title feature for model training.","metadata":{}},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Title']  = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.39955Z","iopub.execute_input":"2022-04-27T22:51:28.399814Z","iopub.status.idle":"2022-04-27T22:51:28.413463Z","shell.execute_reply.started":"2022-04-27T22:51:28.399782Z","shell.execute_reply":"2022-04-27T22:51:28.412446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df[\"Title\"], sub_df['Sex'])","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.414814Z","iopub.execute_input":"2022-04-27T22:51:28.415705Z","iopub.status.idle":"2022-04-27T22:51:28.448647Z","shell.execute_reply.started":"2022-04-27T22:51:28.415657Z","shell.execute_reply":"2022-04-27T22:51:28.44791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can replace many titles with a more common name or classify them as Rare.","metadata":{}},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.449757Z","iopub.execute_input":"2022-04-27T22:51:28.450114Z","iopub.status.idle":"2022-04-27T22:51:28.463247Z","shell.execute_reply.started":"2022-04-27T22:51:28.450085Z","shell.execute_reply":"2022-04-27T22:51:28.462333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5})\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.464694Z","iopub.execute_input":"2022-04-27T22:51:28.465115Z","iopub.status.idle":"2022-04-27T22:51:28.48543Z","shell.execute_reply.started":"2022-04-27T22:51:28.465083Z","shell.execute_reply":"2022-04-27T22:51:28.484759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Handle missing value -  Highly incomplete (Cabin)\nA computed column, Deck, is created because it is slightly more general than Cabin.\n","metadata":{}},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Deck'] = dataset['Cabin'].str.slice(0,1) \n    dataset['Deck'] = dataset['Deck'].map({\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\":6,\"G\":7, \"T\":8})\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(np.int64)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.486498Z","iopub.execute_input":"2022-04-27T22:51:28.486811Z","iopub.status.idle":"2022-04-27T22:51:28.498791Z","shell.execute_reply.started":"2022-04-27T22:51:28.486784Z","shell.execute_reply":"2022-04-27T22:51:28.497598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find the number of null per each columns\nprint('Columns in df with null values:\\n')\nprint(df.isnull().sum())\nprint(\"-\"*30)\n\nprint('Columns in sub_df with null values:\\n')\nprint(sub_df.isnull().sum())\nprint(\"-\"*30)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.500143Z","iopub.execute_input":"2022-04-27T22:51:28.50049Z","iopub.status.idle":"2022-04-27T22:51:28.514438Z","shell.execute_reply.started":"2022-04-27T22:51:28.500461Z","shell.execute_reply":"2022-04-27T22:51:28.513604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create new feature combining existing features\n\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","metadata":{}},{"cell_type":"code","source":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.515516Z","iopub.execute_input":"2022-04-27T22:51:28.515755Z","iopub.status.idle":"2022-04-27T22:51:28.522743Z","shell.execute_reply.started":"2022-04-27T22:51:28.515727Z","shell.execute_reply":"2022-04-27T22:51:28.521747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.524069Z","iopub.execute_input":"2022-04-27T22:51:28.524797Z","iopub.status.idle":"2022-04-27T22:51:28.537543Z","shell.execute_reply.started":"2022-04-27T22:51:28.524764Z","shell.execute_reply":"2022-04-27T22:51:28.536387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop Useless Column\n\nThe **Name, PassengerId**, **Ticket** and **Cabin** should not have a bearing on the analysis. We also do not need the PassengerId feature in the training dataset","metadata":{}},{"cell_type":"markdown","source":"Now we can safely drop the **Name, PassengerId** and **Ticket** feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset","metadata":{}},{"cell_type":"code","source":"#Duplicate variable - Fare & Age\n# for dataset in combine:       \n#     dataset['FareB'] = dataset['Fare']\n#     dataset['AgeB'] = dataset['Age']","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.539212Z","iopub.execute_input":"2022-04-27T22:51:28.539542Z","iopub.status.idle":"2022-04-27T22:51:28.548015Z","shell.execute_reply.started":"2022-04-27T22:51:28.5395Z","shell.execute_reply":"2022-04-27T22:51:28.547197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Binning variable - Fare & Age \nfor dataset in combine:       \n    #Fare Bins/Buckets using qcut or frequency bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.54947Z","iopub.execute_input":"2022-04-27T22:51:28.55022Z","iopub.status.idle":"2022-04-27T22:51:28.577731Z","shell.execute_reply.started":"2022-04-27T22:51:28.55005Z","shell.execute_reply":"2022-04-27T22:51:28.576999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for dataset in combine:\n#     dataset.loc[ dataset['FareB'] <= 7.91, 'FareB'] = 0\n#     dataset.loc[(dataset['FareB'] > 7.91) & (dataset['FareB'] <= 14.454), 'FareB'] = 1\n#     dataset.loc[(dataset['FareB'] > 14.454) & (dataset['FareB'] <= 31), 'FareB']   = 2\n#     dataset.loc[ dataset['FareB'] > 31, 'FareB'] = 3\n#     dataset['FareB'] = dataset['FareB'].astype(int)\n\ncombine = [df, sub_df]\n    \ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.582883Z","iopub.execute_input":"2022-04-27T22:51:28.583313Z","iopub.status.idle":"2022-04-27T22:51:28.605088Z","shell.execute_reply.started":"2022-04-27T22:51:28.583272Z","shell.execute_reply":"2022-04-27T22:51:28.604428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in combine:\n    #Fare Bins/Buckets using qcut or frequency bins\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    #Age Bins/Buckets using cut or value bins\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.606218Z","iopub.execute_input":"2022-04-27T22:51:28.606578Z","iopub.status.idle":"2022-04-27T22:51:28.623171Z","shell.execute_reply.started":"2022-04-27T22:51:28.606544Z","shell.execute_reply":"2022-04-27T22:51:28.622351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for dataset in combine:    \n#     dataset.loc[ dataset['AgeB'] <= 16, 'AgeB'] = 0\n#     dataset.loc[(dataset['AgeB'] > 16) & (dataset['AgeB'] <= 32), 'AgeB'] = 1\n#     dataset.loc[(dataset['AgeB'] > 32) & (dataset['AgeB'] <= 48), 'AgeB'] = 2\n#     dataset.loc[(dataset['AgeB'] > 48) & (dataset['AgeB'] <= 64), 'AgeB'] = 3\n#     dataset.loc[ dataset['AgeB'] > 64, 'AgeB'] = 4\n# train_df.head()\n\ncombine = [df, sub_df]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.624599Z","iopub.execute_input":"2022-04-27T22:51:28.624809Z","iopub.status.idle":"2022-04-27T22:51:28.646263Z","shell.execute_reply.started":"2022-04-27T22:51:28.624784Z","shell.execute_reply":"2022-04-27T22:51:28.645309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert Formats\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.","metadata":{}},{"cell_type":"code","source":"#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in combine:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n    \n\n#define y variable aka target/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(df[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\ndata1_dummy.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.647465Z","iopub.execute_input":"2022-04-27T22:51:28.647696Z","iopub.status.idle":"2022-04-27T22:51:28.690194Z","shell.execute_reply.started":"2022-04-27T22:51:28.64767Z","shell.execute_reply":"2022-04-27T22:51:28.689345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\nsub_df = sub_df.drop(['Name','Ticket','Cabin'], axis=1)\n\ncombine = [df, sub_df]\ndf.shape, sub_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.691675Z","iopub.execute_input":"2022-04-27T22:51:28.692148Z","iopub.status.idle":"2022-04-27T22:51:28.703317Z","shell.execute_reply.started":"2022-04-27T22:51:28.692089Z","shell.execute_reply":"2022-04-27T22:51:28.702456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## check for data type\nprint(\"Training Data:\\n\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.704546Z","iopub.execute_input":"2022-04-27T22:51:28.704805Z","iopub.status.idle":"2022-04-27T22:51:28.728504Z","shell.execute_reply.started":"2022-04-27T22:51:28.704775Z","shell.execute_reply":"2022-04-27T22:51:28.727401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## check for data type\nprint(\"Testing Data:\\n\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.729777Z","iopub.execute_input":"2022-04-27T22:51:28.73019Z","iopub.status.idle":"2022-04-27T22:51:28.753851Z","shell.execute_reply.started":"2022-04-27T22:51:28.730149Z","shell.execute_reply":"2022-04-27T22:51:28.75321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ch3\"></a>\n### 3. EDA (Exploratory Data Analysis) - Analyze, identify patterns, and explore the data Analysis","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n#Survived rate\n\n#Pie Chart\nf,ax=plt.subplots(1,2,figsize=(12,6))\ndf['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\n\n#Bar chart - count\nsns.countplot('Survived',data=df,ax=ax[1])\nax[1].set_title('Survived')\n\nfor p in ax[1].patches:\n    ax[1].annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+20))\n\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:28.754935Z","iopub.execute_input":"2022-04-27T22:51:28.755374Z","iopub.status.idle":"2022-04-27T22:51:29.050063Z","shell.execute_reply.started":"2022-04-27T22:51:28.75532Z","shell.execute_reply":"2022-04-27T22:51:29.04911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The plots show that number of passengers survived the accident. \n- Only 350 out of 891 passengers (38.4%) survived in the training set. ","metadata":{}},{"cell_type":"markdown","source":"'female': 1, 'male': 0","metadata":{}},{"cell_type":"code","source":"eda_df = df.copy()\neda_df['Sex'] =  eda_df['Sex'].map( { 0: 'male',1:'female'} )\neda_df['Deck'] =  eda_df['Deck'].map( {0: 'Missing', 1: 'A', 2: 'B',3:'C',4:'D',5:'E',6:'F',7:'G',8:'T'} )\neda_df['Embarked'] =  eda_df['Embarked'].map( {0: 'Missing', 1: 'S', 2: 'C',3:'Q'} )\neda_df['Title'] =  eda_df['Title'].map( {0: 'Missing', 1: 'Mr', 2: 'Miss',3:'Mrs',4:'Master',5:'Rare'} )\n# eda_df['Survived'] =  eda_df['Survived'].map( {0: 'No',1:'Yes'} )\neda_df['Pclass'] =  eda_df['Pclass'].map( {0: 'Missing',1:'Upper',2:'Middle',3:'Lower'} )\neda_df['IsAlone'] =  eda_df['IsAlone'].map( {0: 'No',1:'Yes'} )","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:29.051681Z","iopub.execute_input":"2022-04-27T22:51:29.052264Z","iopub.status.idle":"2022-04-27T22:51:29.067812Z","shell.execute_reply.started":"2022-04-27T22:51:29.05222Z","shell.execute_reply":"2022-04-27T22:51:29.067193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nfig1 = eda_df.groupby(['Sex', 'Survived'])['Survived'].count().unstack().fillna(0)\nfig2 = eda_df.groupby(['Pclass', 'Survived'])['Survived'].count().unstack().fillna(0)\nfig3 = eda_df.groupby(['IsAlone', 'Survived'])['Survived'].count().unstack().fillna(0)\nfig4 = eda_df.groupby(['Embarked', 'Survived'])['Survived'].count().unstack().fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:29.068944Z","iopub.execute_input":"2022-04-27T22:51:29.069664Z","iopub.status.idle":"2022-04-27T22:51:29.088971Z","shell.execute_reply.started":"2022-04-27T22:51:29.069627Z","shell.execute_reply":"2022-04-27T22:51:29.087985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncolors = ['#2471A3', '#F5B041']\n\ndef stacked_barchart(fig,ax):\n    bottom = np.zeros(len(fig))\n\n    for i, col in enumerate(fig.columns):\n        ax.bar(\n          fig.index, fig[col], bottom=bottom, label=col, color=colors[i])\n        bottom += np.array(fig[col])\n\n    totals = fig.sum(axis=1)\n    y_offset = 4\n    for i, total in enumerate(totals):\n        ax.text(totals.index[i], total + y_offset, round(total), ha='center',\n              weight='bold')\n        \n    # Let's put the annotations inside the bars themselves by using a\n    # negative offset.\n    y_offset = -40\n\n    # For each patch (basically each rectangle within the bar), add a label.\n    for bar in ax.patches:\n        ax.text(\n          # Put the text in the middle of each bar. get_x returns the start\n          # so we add half the width to get to the middle.  \n          bar.get_x() + bar.get_width() / 2,\n          # Vertically, add the height of the bar to the start of the bar,\n          # along with the offset.\n          bar.get_height() + bar.get_y() + y_offset,\n          # This is actual value we'll show.\n          round(bar.get_height()),\n          # Center the labels and style them a bit.\n          ha='center',\n          color='w',\n          weight='bold',\n          size=8\n      )\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:29.090443Z","iopub.execute_input":"2022-04-27T22:51:29.091092Z","iopub.status.idle":"2022-04-27T22:51:29.10102Z","shell.execute_reply.started":"2022-04-27T22:51:29.09106Z","shell.execute_reply":"2022-04-27T22:51:29.100109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax=plt.subplots(1,4,figsize=(22,6))\n\nstacked_barchart(fig1,ax[0])\nax[0].set_title('Survival count by Sex')\nax[0].legend()\n\nstacked_barchart(fig2,ax[1])\nax[1].set_title('Survival count by Pclass')\nax[1].legend()\n\nstacked_barchart(fig3,ax[2])\nax[2].set_title('Survival count by IsAlone')\nax[2].legend()\n\nstacked_barchart(fig4,ax[3])\nax[3].set_title('Survival count by Embarked')\nax[3].legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:29.102491Z","iopub.execute_input":"2022-04-27T22:51:29.102914Z","iopub.status.idle":"2022-04-27T22:51:29.810262Z","shell.execute_reply.started":"2022-04-27T22:51:29.102883Z","shell.execute_reply":"2022-04-27T22:51:29.809618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df[['AgeBin', 'Survived']].groupby(['AgeBin'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n## Analyze FamilySize feature with survived\ndisplay(df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n## Analyze Pclass feature with survived\ndisplay(df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n## Analyze sex feature with survived\ndisplay(df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n## Analyze SibSp feature with survived\ndisplay(df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n## Analyze Parch feature with survived\ndisplay(df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n## Analyze IsAlone feature with survived\ndisplay(df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean().sort_values(by='Survived', ascending=False))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:29.811552Z","iopub.execute_input":"2022-04-27T22:51:29.811961Z","iopub.status.idle":"2022-04-27T22:51:29.89052Z","shell.execute_reply.started":"2022-04-27T22:51:29.811928Z","shell.execute_reply":"2022-04-27T22:51:29.889595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Sex\n- The number of men on board the ship is much higher than the number of women, but the number of women saved is more than twice that of the number of males survived.\n- The survival rates for a women on the ship is around 75% while that for men in around 19%.\n\n##### Pclass\n- Passenegers Of Pclass 1 has a very high priority to survive.\n- The number of Passengers in Pclass 3 were a lot higher than Pclass 1 and Pclass 2, but still the number of survival from pclass 3 is low compare to them.\n- Pclass 1 %survived is around 63%, for Pclass2 is around 48%, and Pclass3 survived is around 25%","metadata":{}},{"cell_type":"code","source":"pd.crosstab([eda_df.Sex,eda_df.Survived],eda_df.Pclass,margins=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:29.892035Z","iopub.execute_input":"2022-04-27T22:51:29.892362Z","iopub.status.idle":"2022-04-27T22:51:29.939777Z","shell.execute_reply.started":"2022-04-27T22:51:29.892321Z","shell.execute_reply":"2022-04-27T22:51:29.939012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Pclass','Survived',hue='Sex',data=eda_df, palette='Set2',order=['Upper','Middle','Lower'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:29.942065Z","iopub.execute_input":"2022-04-27T22:51:29.942632Z","iopub.status.idle":"2022-04-27T22:51:30.596648Z","shell.execute_reply.started":"2022-04-27T22:51:29.942581Z","shell.execute_reply":"2022-04-27T22:51:30.595451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Female from Upper class is about 95-96% survived. Only 3 out of 94 Women from Upper class died.\n- Female Upper class has high priority to survive\n- Lower class female has more survived rate than Upper class male.","metadata":{}},{"cell_type":"markdown","source":"#### Features Correlation with Survived:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(df.corr()[['Survived']].sort_values(by='Survived', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with Survived', fontdict={'fontsize':18}, pad=16);\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:30.597956Z","iopub.execute_input":"2022-04-27T22:51:30.598241Z","iopub.status.idle":"2022-04-27T22:51:31.016043Z","shell.execute_reply.started":"2022-04-27T22:51:30.598198Z","shell.execute_reply":"2022-04-27T22:51:31.015437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Sex is positively corrlated with Survived (with a Person's correlation coefficient of 0.54) ; Female is more likely to survive\n- Pclass is negatively correlated with Survived(with a Pearson‚Äôs correlation coefficient of -0.34) ; Obviously, better the ticket class (1 = 1st/Upper ; 2 = 2nd/Middle; 3 = 3rd/Lower), higher the chance of survival.\n\n- Those important feature for prediction the Survived people\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Correlation between features:","metadata":{}},{"cell_type":"code","source":"# get correlations\ndf_corr = df.corr()\n\nfig, ax = plt.subplots(figsize=(12, 10))\n# mask\nmask = np.triu(np.ones_like(df_corr, dtype=np.bool))\n# adjust mask and df\nmask = mask[1:, :-1]\ncorr = df_corr.iloc[1:,:-1].copy()\n\n# color map\n# cmap = sb.diverging_palette(0, 230, 90, 60, as_cmap=True)\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", \n           linewidths=5, cmap='BrBG', vmin=-1, vmax=1, \n           cbar_kws={\"shrink\": .8}, square=True)\n# ticks\nyticks = [i.upper() for i in corr.index]\nxticks = [i.upper() for i in corr.columns]\nplt.yticks(plt.yticks()[0], labels=yticks, rotation=0)\nplt.xticks(plt.xticks()[0], labels=xticks)\n# title\ntitle = 'Correlation Matrix\\n'\nplt.title(title, loc='left', fontsize=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:31.017015Z","iopub.execute_input":"2022-04-27T22:51:31.017656Z","iopub.status.idle":"2022-04-27T22:51:32.011034Z","shell.execute_reply.started":"2022-04-27T22:51:31.017614Z","shell.execute_reply":"2022-04-27T22:51:32.010047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the analysis, passengers were more likely to survive if:\n\n- they had a high class ticket\n- they were women\n- they were young\n- they embarked from Cherbourg\n\nOn the contrary, being a third class old man from Southampton lowered your chances of survival.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['AgeBin'], axis=1)\ndf = df.drop(['FareBin'], axis=1)\nsub_df = sub_df.drop(['AgeBin'], axis=1)\nsub_df = sub_df.drop(['FareBin'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:32.01235Z","iopub.execute_input":"2022-04-27T22:51:32.012789Z","iopub.status.idle":"2022-04-27T22:51:32.023491Z","shell.execute_reply.started":"2022-04-27T22:51:32.012747Z","shell.execute_reply":"2022-04-27T22:51:32.022586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ch4\"></a>\n### 4. Acquire training and testing data","metadata":{}},{"cell_type":"code","source":"X_train = df.drop(\"Survived\", axis=1)\nY_train = df[\"Survived\"]\nX_test  = sub_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:32.024676Z","iopub.execute_input":"2022-04-27T22:51:32.025479Z","iopub.status.idle":"2022-04-27T22:51:32.038991Z","shell.execute_reply.started":"2022-04-27T22:51:32.025432Z","shell.execute_reply":"2022-04-27T22:51:32.038142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ch5\"></a>\n### 5. Model, predict and solve the problem\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\nLogistic Regression\nKNN or k-Nearest Neighbors\nSupport Vector Machines\nNaive Bayes classifier\nDecision Tree\nRandom Forrest\nPerceptron\nArtificial neural network\nRVM or Relevance Vector Machine","metadata":{}},{"cell_type":"markdown","source":"#### Split Training and Testing Data","metadata":{}},{"cell_type":"code","source":"#define y variable aka target/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:32.040177Z","iopub.execute_input":"2022-04-27T22:51:32.040859Z","iopub.status.idle":"2022-04-27T22:51:32.04936Z","shell.execute_reply.started":"2022-04-27T22:51:32.040823Z","shell.execute_reply":"2022-04-27T22:51:32.048401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(df[data1_x_calc], df[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(df[data1_x_bin], df[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], df[Target], random_state = 0)\n\n\nprint(\"Data1 Shape: {}\".format(df.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\n\ntrain1_x_bin.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:32.051056Z","iopub.execute_input":"2022-04-27T22:51:32.051931Z","iopub.status.idle":"2022-04-27T22:51:32.080275Z","shell.execute_reply.started":"2022-04-27T22:51:32.05188Z","shell.execute_reply":"2022-04-27T22:51:32.07967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()    \n    ]\n\n\n\n#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = df[Target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, df[data1_x_bin], df[Target], cv  = cv_split, return_train_score=True)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(df[data1_x_bin], df[Target])\n    MLA_predict[MLA_name] = alg.predict(df[data1_x_bin])\n    \n    row_index+=1\n\n    \n#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:32.081204Z","iopub.execute_input":"2022-04-27T22:51:32.081608Z","iopub.status.idle":"2022-04-27T22:51:51.74353Z","shell.execute_reply.started":"2022-04-27T22:51:32.081577Z","shell.execute_reply":"2022-04-27T22:51:51.742594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:51.745Z","iopub.execute_input":"2022-04-27T22:51:51.745447Z","iopub.status.idle":"2022-04-27T22:51:52.221455Z","shell.execute_reply.started":"2022-04-27T22:51:51.745404Z","shell.execute_reply":"2022-04-27T22:51:52.220537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Rank the model\nWe can use binary cross entropy or logistic loss as the loss function and any metric like accuracy or/and ROC AUC score as a metric to evaluate the results.\n\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set.","metadata":{}},{"cell_type":"code","source":"# First XGBoost model for Pima Indians dataset\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# fit model\nmodel = XGBClassifier()\nmodel.fit(X_train, Y_train)\n# make predictions for test data\nY_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:52.22262Z","iopub.execute_input":"2022-04-27T22:51:52.222867Z","iopub.status.idle":"2022-04-27T22:51:52.538824Z","shell.execute_reply.started":"2022-04-27T22:51:52.222826Z","shell.execute_reply":"2022-04-27T22:51:52.537967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ch6\"></a>\n### 6. Visualize, report, and present the problem solving steps and final solution","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ch7\"></a>\n### 7. Submit the results","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": sub_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:51:52.540052Z","iopub.execute_input":"2022-04-27T22:51:52.540502Z","iopub.status.idle":"2022-04-27T22:51:52.557083Z","shell.execute_reply.started":"2022-04-27T22:51:52.540462Z","shell.execute_reply":"2022-04-27T22:51:52.556277Z"},"trusted":true},"execution_count":null,"outputs":[]}]}